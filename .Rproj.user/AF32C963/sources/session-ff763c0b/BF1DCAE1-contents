# Trying out a penalized regression approach for misclassification models.

# Load necessary packages.
library(COMBO)
library(dplyr)
library(ggplot2)

save_directory <- "C:/Users/kimho/Dropbox/Misclassification/Code/EM_Multinom_Misclass/Penalized_Regression/misclassification_pl/test_sim_studies_2024-06-10/"

################################################################################

# Generate data.
set.seed(123)
n <- 2000
x_mu <- 0
x_sigma <- 1
z_shape <- 1

true_beta <- matrix(c(1, -2), ncol = 1)
true_gamma <- matrix(c(.5, 1, -2, .8), nrow = 2, byrow = FALSE)

my_data <- COMBO_data(sample_size = n,
                      x_mu = x_mu, x_sigma = x_sigma,
                      z_shape = z_shape,
                      beta = true_beta, gamma = true_gamma)

table(my_data[["obs_Y"]], my_data[["true_Y"]])
prop.table(table(my_data[["obs_Y"]], my_data[["true_Y"]]), 2)

################################################################################

# Write a penalized likelihood function.
penalized_likelihood <- function(regression_parameters, lambda, 
                                 x, z, ystar){
  
  beta_terms <- regression_parameters[1:2]
  gamma_terms <- regression_parameters[3:6]
  
  pi_terms <- exp(beta_terms[1] + beta_terms[2] * x) /
    (1 + exp(beta_terms[1] + beta_terms[2] * x))
  
  pistar_terms_1 <- exp(gamma_terms[1] + gamma_terms[2] * z) /
    (1 + exp(gamma_terms[1] + gamma_terms[2] * z))
  pistar_terms_2 <- exp(gamma_terms[3] + gamma_terms[4] * z) /
    (1 + exp(gamma_terms[4] + gamma_terms[4] * z))
  
  ystar_01_matrix <- matrix(c(ifelse(ystar == 1, 1, 0), 
                              ifelse(ystar == 2, 1, 0)),
                            ncol = 2, byrow = FALSE)
  
  obs_loglikelihood <- sum((ystar_01_matrix[,1] *
                              log(pistar_terms_1 * pi_terms +
                                    pistar_terms_2 * (1 - pi_terms))) +
                             (ystar_01_matrix[,2] *
                                log((1 - pistar_terms_1) * pi_terms +
                                      (1 - pistar_terms_2) * (1 - pi_terms))))
  
  mean_pistar11 <- mean(pistar_terms_1)
  mean_pistar22 <- mean(1 - pistar_terms_2)
  penalization_term <- lambda * (log(mean_pistar11) + log(mean_pistar22))
  
  loglike_sum <- obs_loglikelihood + penalization_term
  
  return(loglike_sum)
}


# Write a penalized likelihood function * -1.
negative_penalized_likelihood <- function(regression_parameters, lambda, 
                                          x, z, ystar){
  
  beta_terms <- regression_parameters[1:2]
  gamma_terms <- regression_parameters[3:6]
  
  pi_terms <- exp(beta_terms[1] + beta_terms[2] * x) /
    (1 + exp(beta_terms[1] + beta_terms[2] * x))
  
  pistar_terms_1 <- exp(gamma_terms[1] + gamma_terms[2] * z) /
    (1 + exp(gamma_terms[1] + gamma_terms[2] * z))
  pistar_terms_2 <- exp(gamma_terms[3] + gamma_terms[4] * z) /
    (1 + exp(gamma_terms[4] + gamma_terms[4] * z))
  
  ystar_01_matrix <- matrix(c(ifelse(ystar == 1, 1, 0), 
                              ifelse(ystar == 2, 1, 0)),
                            ncol = 2, byrow = FALSE)
  
  obs_loglikelihood <- sum((ystar_01_matrix[,1] *
                              log(pistar_terms_1 * pi_terms +
                                    pistar_terms_2 * (1 - pi_terms))) +
                             (ystar_01_matrix[,2] *
                                log((1 - pistar_terms_1) * pi_terms +
                                      (1 - pistar_terms_2) * (1 - pi_terms))))
  
  mean_pistar11 <- mean(pistar_terms_1)
  mean_pistar22 <- mean(1 - pistar_terms_2)
  penalization_term <- lambda * (log(mean_pistar11) + log(mean_pistar22))
  
  loglike_sum <- -1 * (obs_loglikelihood + penalization_term)
  
  return(loglike_sum)
}

################################################################################

# Test it out

# True parameter starting values.
true_start <- c(true_beta, c(true_gamma))
true_start_optim <- optim(true_start, fn = penalized_likelihood,
                          lambda = 1,
                          x = my_data$x, z = my_data$z, ystar = my_data$obs_Y,
                          #method = "BFGS",
                          control = list(fnscale = -1, maxit = 1000))

true_start_nlm <- nlm(negative_penalized_likelihood, p = true_start,
                      lambda = 1,
                      x = my_data$x, z = my_data$z, ystar = my_data$obs_Y)


# Naive starting values. 
ystar01 <- my_data$obs_Y_matrix[,1]
x = my_data$x
naive_lm <- glm(ystar01 ~ x, family = "binomial"(link = "logit"))

naive_start <- c(unname(coef(naive_lm)), 1, 0, -1, 0)
naive_start_optim <- optim(naive_start, fn = penalized_likelihood,
                           lambda = 1,
                           x = my_data$x, z = my_data$z, ystar = my_data$obs_Y,
                           #method = "BFGS",
                           control = list(fnscale = -1, maxit = 1000))

naive_start_nlm <- nlm(negative_penalized_likelihood, p = naive_start,
                       lambda = 1,
                       x = my_data$x, z = my_data$z, ystar = my_data$obs_Y)


################################################################################

# Test for multiple lambdas with average penalty.
## "Small n large misclassification rate" setting.

lambda_list <- c(0, .25, .5, .75, 1, 2, 5, 10)
n_sim <- 100

# This is close to simulation setting #1 in the COMBO paper. 
n <- 1000
x_mu <- 0
x_sigma <- 1
z_shape <- 1

true_beta <- matrix(c(1, -2), ncol = 1)
true_gamma <- matrix(c(.5, 1, -.5, -1), nrow = 2, byrow = FALSE)
true_parameters <- c(true_beta, c(true_gamma))

lambda_tracker <- NULL
param_results_tracker <- NULL

set.seed(12345)
for(i in 1:length(lambda_list)){
  
  current_lambda <- lambda_list[i]
  
  for(j in 1:n_sim){
    
    my_data_i <- COMBO_data(sample_size = n,
                            x_mu = x_mu, x_sigma = x_sigma,
                            z_shape = z_shape,
                            beta = true_beta, gamma = true_gamma)
    
    table(my_data_i[["obs_Y"]], my_data_i[["true_Y"]])
    prop.table(table(my_data_i[["obs_Y"]], my_data_i[["true_Y"]]), 2)
    
    ystar01 <- my_data_i$obs_Y_matrix[,1]
    x = my_data_i$x
    naive_lm_i  <- glm(ystar01 ~ x, family = "binomial"(link = "logit"))
    
    naive_start_i <- c(unname(coef(naive_lm_i)), 1, 0, -1, 0)
    
    naive_start_optim_i <- optim(naive_start, fn = penalized_likelihood,
                                 lambda = current_lambda,
                                 x = my_data_i$x, z = my_data_i$z, ystar = my_data_i$obs_Y,
                                 #method = "BFGS",
                                 control = list(fnscale = -1, maxit = 1000))
    
    lambda_tracker <- c(lambda_tracker, rep(current_lambda, 6))
    param_results_tracker <- c(param_results_tracker,
                               naive_start_optim_i$par)
    
    print(paste0("Done with simulation ", j,
                 " for lambda value ", current_lambda, "!"))
    
  }
}

parameter_names <- c("beta_0", "beta_x",
                     "gamma_110", "gamma_11z",
                     "gamma_120", "gamma_12z")

results_df <- data.frame(lambda = lambda_tracker,
                         estimate = param_results_tracker,
                         parameter = rep(parameter_names,
                                         length(lambda_list) * n_sim),
                         true_value = rep(true_parameters,
                                          length(lambda_list) * n_sim))

#save(results_df,
#     file = paste0(save_directory, "binaryY_test_s1_2024-06-10.RData"))
load(paste0(save_directory, "binaryY_test_s1_2024-06-10.RData"))

print(results_df %>%
  group_by(parameter, lambda) %>%
  mutate(bias = true_value - estimate) %>%
  summarise(mean_estimate = mean(estimate),
            sd_estimate = sd(estimate),
            avg_bias = mean(bias),
            true_value = mean(true_value)) %>%
  mutate(rmse = avg_bias^2 + sd_estimate^2) %>%
  mutate(lowest_rmse = ifelse(rmse == min(rmse), "*", "")) %>%
  mutate(lambda_with_lowest_rmse = ifelse(lowest_rmse == "*", lambda, "")) %>%
  ungroup(),
  n = 100)


results_df %>%
  group_by(parameter) %>%
  mutate(bias = true_value - estimate) %>%
  summarise(mean_estimate = mean(estimate),
            sd_estimate = sd(estimate),
            avg_bias = mean(bias),
            true_value = mean(true_value)) %>%
  mutate(rmse = avg_bias^2 + sd_estimate^2) %>%
  ungroup()

ggplot(data = results_df) +
  geom_density(aes(x = estimate, fill = as.factor(lambda)),
                   alpha = .8) +
  geom_vline(aes(xintercept = true_value)) +
  facet_wrap(~parameter, scales = "free") +
  theme_minimal()

################################################################################

# Test for multiple lambdas with average penalty.
## "Large n small misclassification rate" setting.

lambda_list2 <- c(0, .25, .5, .75, 1, 2)
n_sim <- 50

# This is close to simulation setting #2 in the COMBO paper. 
n <- 10000
x_mu <- 0
x_sigma <- 1
z_shape <- 3

true_beta <- matrix(c(1, -2), ncol = 1)
true_gamma <- matrix(c(1, 1.2, -1.3, -1), nrow = 2, byrow = FALSE)
true_parameters <- c(true_beta, c(true_gamma))

lambda_tracker2 <- NULL
param_results_tracker2 <- NULL

set.seed(123)
for(i in 1:length(lambda_list2)){
  
  current_lambda <- lambda_list2[i]
  
  for(j in 1:n_sim){
    
    my_data_i <- COMBO_data(sample_size = n,
                            x_mu = x_mu, x_sigma = x_sigma,
                            z_shape = z_shape,
                            beta = true_beta, gamma = true_gamma)
    
    table(my_data_i[["obs_Y"]], my_data_i[["true_Y"]])
    prop.table(table(my_data_i[["obs_Y"]], my_data_i[["true_Y"]]), 2)
    
    ystar01 <- my_data_i$obs_Y_matrix[,1]
    x = my_data_i$x
    naive_lm_i  <- glm(ystar01 ~ x, family = "binomial"(link = "logit"))
    
    naive_start_i <- c(unname(coef(naive_lm_i)), 1, 0, -1, 0)
    
    naive_start_optim_i <- optim(naive_start, fn = penalized_likelihood,
                                 lambda = current_lambda,
                                 x = my_data_i$x, z = my_data_i$z,
                                 ystar = my_data_i$obs_Y,
                                 #method = "BFGS",
                                 control = list(fnscale = -1, maxit = 1000))
    
    lambda_tracker2 <- c(lambda_tracker2, rep(current_lambda, 6))
    param_results_tracker2 <- c(param_results_tracker2,
                                naive_start_optim_i$par)
    
    print(paste0("Done with simulation ", j,
                 " for lambda value ", current_lambda, "!"))
    
  }
}

parameter_names <- c("beta_0", "beta_x",
                     "gamma_110", "gamma_11z",
                     "gamma_120", "gamma_12z")

results_df2 <- data.frame(lambda = lambda_tracker2,
                          estimate = param_results_tracker2,
                          parameter = rep(parameter_names,
                                          length(lambda_list2) * n_sim),
                          true_value = rep(true_parameters,
                                           length(lambda_list2) * n_sim))

#save(results_df2,
#     file = paste0(save_directory, "binaryY_test_s2_2024-06-10.RData"))

load(paste0(save_directory, "binaryY_test_s2_2024-06-10.RData"))

print(results_df2 %>%
        group_by(parameter, lambda) %>%
        mutate(bias = true_value - estimate) %>%
        summarise(mean_estimate = mean(estimate),
                  sd_estimate = sd(estimate),
                  avg_bias = mean(bias),
                  true_value = mean(true_value)) %>%
        mutate(rmse = avg_bias^2 + sd_estimate^2) %>%
        mutate(lowest_rmse = ifelse(rmse == min(rmse), "*", "")) %>%
        mutate(lambda_with_lowest_rmse = ifelse(lowest_rmse == "*", lambda, "")) %>%
        ungroup(),
      n = 100)


ggplot(data = results_df2) +
  geom_density(aes(x = estimate, fill = as.factor(lambda)),
               alpha = .8) +
  geom_vline(aes(xintercept = true_value)) +
  facet_wrap(~parameter, scales = "free") +
  theme_minimal()

################################################################################

# What if the start parameters are bad?

lambda_list3 <- c(0, .25, .5, .75, 1, 2, 5, 10)
n_sim <- 100

# This is close to simulation setting #1 in the COMBO paper. 
n <- 1000
x_mu <- 0
x_sigma <- 1
z_shape <- 1

true_beta <- matrix(c(1, -2), ncol = 1)
true_gamma <- matrix(c(.5, 1, -.5, -1), nrow = 2, byrow = FALSE)
true_parameters <- c(true_beta, c(true_gamma))

lambda_tracker3 <- NULL
param_results_tracker3 <- NULL
convergence_tracker3 <- NULL

set.seed(123)
for(i in 1:length(lambda_list3)){
  
  current_lambda <- lambda_list3[i]
  
  for(j in 1:n_sim){
    
    my_data_i <- COMBO_data(sample_size = n,
                            x_mu = x_mu, x_sigma = x_sigma,
                            z_shape = z_shape,
                            beta = true_beta, gamma = true_gamma)
    
    bad_start <- rep(0, 6)
    
    bad_start_optim_i <- optim(bad_start, fn = penalized_likelihood,
                               lambda = current_lambda,
                               x = my_data_i$x, z = my_data_i$z,
                               ystar = my_data_i$obs_Y,
                               #method = "BFGS",
                               control = list(fnscale = -1, maxit = 5000))
    
    lambda_tracker3 <- c(lambda_tracker3, rep(current_lambda, 6))
    param_results_tracker3 <- c(param_results_tracker3,
                                bad_start_optim_i$par)
    convergence_tracker3 <- c(convergence_tracker3,
                              rep(bad_start_optim_i$convergence, 6))
    
    print(paste0("Done with simulation ", j,
                 " for lambda value ", current_lambda, "!"))
    
  }
}

parameter_names <- c("beta_0", "beta_x",
                     "gamma_110", "gamma_11z",
                     "gamma_120", "gamma_12z")

results_df3 <- data.frame(lambda = lambda_tracker3,
                          estimate = param_results_tracker3,
                          parameter = rep(parameter_names,
                                          length(lambda_list3) * n_sim),
                          true_value = rep(true_parameters,
                                           length(lambda_list3) * n_sim),
                          convergence = convergence_tracker3)

#save(results_df3,
#     file = paste0(save_directory, "binaryY_test_s3_2024-06-10.RData"))

load(paste0(save_directory, "binaryY_test_s3_2024-06-10.RData"))

table(results_df3$convergence) # 0 means converged.

print(results_df3 %>%
        group_by(parameter, lambda) %>%
        mutate(bias = true_value - estimate) %>%
        summarise(mean_estimate = mean(estimate),
                  sd_estimate = sd(estimate),
                  avg_bias = mean(bias),
                  true_value = mean(true_value)) %>%
        mutate(rmse = avg_bias^2 + sd_estimate^2) %>%
        mutate(lowest_rmse = ifelse(rmse == min(rmse), "*", "")) %>%
        mutate(lambda_with_lowest_rmse = ifelse(lowest_rmse == "*", lambda, "")) %>%
        ungroup(),
      n = 100, width = Inf)

# Way worse results. Bimodal results (in graph) except for largest lambda.

ggplot(data = results_df3) +
  geom_density(aes(x = estimate, fill = as.factor(lambda)),
               alpha = .8) +
  geom_vline(aes(xintercept = true_value)) +
  facet_wrap(~parameter, scales = "free") +
  theme_minimal()

################################################################################

# What if the start parameters are bad and there is a lot of misclassification for Y = 2?

lambda_list4 <- c(0, .5, 1, 2, 5, 10, 15)
n_sim <- 100

# This is close to simulation setting #1 in the COMBO paper. 
n <- 1000
x_mu <- 0
x_sigma <- 1
z_shape <- 2.5

true_beta <- matrix(c(1, -2), ncol = 1)
true_gamma <- matrix(c(.5, 1, 2.5, -1), nrow = 2, byrow = FALSE)
true_parameters <- c(true_beta, c(true_gamma))

lambda_tracker4 <- NULL
param_results_tracker4 <- NULL
convergence_tracker4 <- NULL

set.seed(123)
for(i in 1:length(lambda_list4)){
  
  current_lambda <- lambda_list4[i]
  
  for(j in 1:n_sim){
    
    my_data_i <- COMBO_data(sample_size = n,
                            x_mu = x_mu, x_sigma = x_sigma,
                            z_shape = z_shape,
                            beta = true_beta, gamma = true_gamma)
    
    table(my_data_i[["obs_Y"]], my_data_i[["true_Y"]])
    prop.table(table(my_data_i[["obs_Y"]], my_data_i[["true_Y"]]), 2)
    
    bad_start <- rep(0, 6)
    
    bad_start_optim_i <- optim(bad_start, fn = penalized_likelihood,
                               lambda = current_lambda,
                               x = my_data_i$x, z = my_data_i$z,
                               ystar = my_data_i$obs_Y,
                               #method = "BFGS",
                               control = list(fnscale = -1, maxit = 5000))
    
    lambda_tracker4 <- c(lambda_tracker4, rep(current_lambda, 6))
    param_results_tracker4 <- c(param_results_tracker4,
                                bad_start_optim_i$par)
    convergence_tracker4 <- c(convergence_tracker4,
                              rep(bad_start_optim_i$convergence, 6))
    
    
    print(paste0("Done with simulation ", j,
                 " for lambda value ", current_lambda, "!"))
    
  }
}

parameter_names <- c("beta_0", "beta_x",
                     "gamma_110", "gamma_11z",
                     "gamma_120", "gamma_12z")

results_df4 <- data.frame(lambda = lambda_tracker4,
                          estimate = param_results_tracker4,
                          parameter = rep(parameter_names,
                                          length(lambda_list4) * n_sim),
                          true_value = rep(true_parameters,
                                           length(lambda_list4) * n_sim),
                          convergence = convergence_tracker4)

table(convergence_tracker4)
table(lambda_tracker4, convergence_tracker4)

#save(results_df4,
#     file = paste0(save_directory, "binaryY_test_s4_2024-06-10.RData"))

load(paste0(save_directory, "binaryY_test_s4_2024-06-10.RData"))

print(results_df4 %>%
        group_by(parameter, lambda) %>%
        mutate(bias = true_value - estimate) %>%
        summarise(mean_estimate = mean(estimate),
                  sd_estimate = sd(estimate),
                  avg_bias = mean(bias),
                  true_value = mean(true_value)) %>%
        mutate(rmse = avg_bias^2 + sd_estimate^2) %>%
        mutate(lowest_rmse = ifelse(rmse == min(rmse), "*", "")) %>%
        mutate(lambda_with_lowest_rmse = ifelse(lowest_rmse == "*", lambda, "")) %>%
        ungroup(),
      n = 100)


ggplot(data = results_df4 %>% filter(lambda != 15)) +
  geom_density(aes(x = estimate, fill = as.factor(lambda)),
               alpha = .8) +
  geom_vline(aes(xintercept = true_value)) +
  facet_wrap(~parameter, scales = "free") +
  theme_minimal()
